{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of9d8jp_WPs-"
      },
      "source": [
        "**WM811K - Autoencoder â†’ Classifier**\n",
        "(using C++-processed data: wafer + dist_to_center + bad_mask)\n",
        "\n",
        "What this script does:\n",
        "1) Builds datasets from the C++ outputs:\n",
        "   - processed_images/<label>/<file>.png\n",
        "   - processed_aux/dist/<label>/<file>.png\n",
        "   - processed_masks/bad/<label>/<file>.png\n",
        "   - converted_images/metadata.csv\n",
        "2) Pretrains an autoencoder on ALL images (labeled + unlabeled)\n",
        "3) Reuses the encoder for a classifier (9 classes), trained on labeled data\n",
        "4) Evaluates with macro-F1; saves SavedModels;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNQRP8HrVrHI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1mm4TgOVulo"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow:\", tf.__version__)\n",
        "DEVICE = 'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWtZE3kXW4sx"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImhBEJGgVxuv"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE   = 64   # must match the C++ preprocessor\n",
        "BATCH_AE   = 512  # AE pretrain batch size\n",
        "BATCH_CLF  = 512  # classifier batch size\n",
        "EPOCHS_AE  = 3\n",
        "EPOCHS_CLF = 3\n",
        "LIMIT      = None   # 20000 for quick run, or None for all\n",
        "\n",
        "ROOT_PROC = Path('processed_images')\n",
        "ROOT_DIST = Path('processed_aux/dist')\n",
        "ROOT_BAD  = Path('processed_masks/bad')\n",
        "META_CSV  = Path('extracted_images/metadata.csv')\n",
        "\n",
        "CLASSES  = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-Full','None']\n",
        "CLS2ID   = {c:i for i,c in enumerate(CLASSES)}\n",
        "N_CLASSES = len(CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqb4BRDNXgyy"
      },
      "source": [
        "# Label normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1j-FaTfV0SO"
      },
      "outputs": [],
      "source": [
        "def canon_label(s):\n",
        "    if s is None:\n",
        "        return 'Unlabeled'\n",
        "    s = str(s).strip().strip('_').replace('_','-').lower()\n",
        "    table = {\n",
        "        'center':'Center','donut':'Donut','edge-loc':'Edge-Loc','edge-ring':'Edge-Ring',\n",
        "        'loc':'Loc','random':'Random','scratch':'Scratch','near-full':'Near-Full',\n",
        "        'none':'None','unlabeled':'Unlabeled'\n",
        "    }\n",
        "    return table.get(s, 'Unlabeled')\n",
        "\n",
        "def is_labeled(lbl):\n",
        "    return lbl in CLS2ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzTMj22CXkBT"
      },
      "source": [
        "# Build dataframe of triplets (wafer, dist, bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyO95Rd4V3xw"
      },
      "outputs": [],
      "source": [
        "assert META_CSV.exists(), \"metadata.csv not found at converted_images/metadata.csv\"\n",
        "meta = pd.read_csv(META_CSV)\n",
        "\n",
        "for col in ['file','label','split']:\n",
        "    if col not in meta.columns:\n",
        "        raise ValueError(f\"metadata.csv missing required column: {col}\")\n",
        "\n",
        "meta['label'] = meta['label'].apply(canon_label)\n",
        "\n",
        "def triplet_paths(rel):\n",
        "    p = Path(rel)\n",
        "    return (ROOT_PROC/p, ROOT_DIST/p, ROOT_BAD/p)\n",
        "\n",
        "rows = []\n",
        "for _, r in meta.iterrows():\n",
        "    w, d, b = triplet_paths(r['file'])\n",
        "    rows.append([str(w), str(d), str(b), r['label'], r['split']])\n",
        "df = pd.DataFrame(rows, columns=['wafer','dist','bad','label','split'])\n",
        "\n",
        "mask = df['wafer'].apply(lambda p: Path(p).exists()) \\\n",
        "     & df['dist'].apply(lambda p: Path(p).exists()) \\\n",
        "     & df['bad'].apply(lambda p: Path(p).exists())\n",
        "df = df[mask].reset_index(drop=True)\n",
        "\n",
        "df_l = df[df['label'].apply(is_labeled)].reset_index(drop=True)\n",
        "df_u = df[~df['label'].apply(is_labeled)].reset_index(drop=True)\n",
        "\n",
        "train_df = df_l[df_l['split'].astype(str).str.lower().str.startswith('train')].reset_index(drop=True)\n",
        "test_df  = df_l[df_l['split'].astype(str).str.lower().str.startswith('test')].reset_index(drop=True)\n",
        "\n",
        "if len(train_df) == 0 or len(test_df) == 0:\n",
        "    if len(df_l) == 0:\n",
        "        print(\"WARNING: No labeled samples found. Classifier will be skipped.\")\n",
        "        train_df = pd.DataFrame(columns=df_l.columns)\n",
        "        test_df  = pd.DataFrame(columns=df_l.columns)\n",
        "    else:\n",
        "        print(\"INFO: No usable train/test in metadata. Creating a stratified 80/20 split.\")\n",
        "        train_df, test_df = train_test_split(\n",
        "            df_l, test_size=0.2, random_state=42, stratify=df_l['label']\n",
        "        )\n",
        "        train_df = train_df.reset_index(drop=True)\n",
        "        test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "if LIMIT is not None:\n",
        "    train_df = train_df.iloc[:LIMIT].reset_index(drop=True)\n",
        "    test_df  = test_df.iloc[:min(LIMIT, len(test_df))].reset_index(drop=True)\n",
        "\n",
        "print(f\"Triplets: {len(df)} | Train labeled: {len(train_df)} | Test labeled: {len(test_df)} | Unlabeled: {len(df_u)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1oQsKWnXr45"
      },
      "source": [
        "# Image readers and loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giWq4n2wV6aC"
      },
      "outputs": [],
      "source": [
        "def read_gray64(path):\n",
        "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(path)\n",
        "    if img.shape[0] != IMG_SIZE or img.shape[1] != IMG_SIZE:\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "    return img\n",
        "\n",
        "def load_triplet_row(row):\n",
        "    \"\"\"Return (H,W,3) float32 in [0,1], channels = [wafer, dist, bad]\"\"\"\n",
        "    w = read_gray64(row['wafer'])\n",
        "    d = read_gray64(row['dist'])\n",
        "    b = read_gray64(row['bad'])\n",
        "\n",
        "    w = (w // 127).astype(np.float32) / 2.0\n",
        "    d = (d.astype(np.float32) / 255.0)\n",
        "    b = (b.astype(np.float32) / 255.0)\n",
        "\n",
        "    x = np.stack([w, d, b], axis=-1)\n",
        "    return x\n",
        "\n",
        "def gen_x(frame):\n",
        "    for _, r in frame.iterrows():\n",
        "        yield load_triplet_row(r)\n",
        "\n",
        "def gen_xy(frame):\n",
        "    for _, r in frame.iterrows():\n",
        "        yield load_triplet_row(r), CLS2ID[r['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfIf4L62XuPG"
      },
      "source": [
        "# tf.data pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu_8wPRZV8N4"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def make_ds_all(frame, batch):\n",
        "    \"\"\"For Autoencoder: yields (x, x) so Keras has targets.\"\"\"\n",
        "    ds_x = tf.data.Dataset.from_generator(\n",
        "        lambda: gen_x(frame),\n",
        "        output_signature=tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n",
        "    )\n",
        "    ds = ds_x.map(lambda x: (x, x), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.shuffle(4096).batch(batch).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def make_ds_xy(frame, batch, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        lambda: gen_xy(frame),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "        )\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(4096)\n",
        "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGSKs4QYZH4m"
      },
      "outputs": [],
      "source": [
        "ae_df = pd.concat([df_l, df_u], axis=0).reset_index(drop=True)\n",
        "ae_ds = make_ds_all(ae_df, BATCH_AE)\n",
        "train_ds = make_ds_xy(train_df, BATCH_CLF, shuffle=True) if len(train_df) else None\n",
        "test_ds  = make_ds_xy(test_df,  BATCH_CLF, shuffle=False) if len(test_df) else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TarvPDIyXxHS"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoo7K45aV-JN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='input')\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(32, 3, padding='same', activation='relu')(inp)\n",
        "x = layers.MaxPooling2D()(x)  # 32x32\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = layers.MaxPooling2D()(x)  # 16x16\n",
        "z = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "z = layers.MaxPooling2D()(z)  # 8x8 latent\n",
        "\n",
        "# Decoder\n",
        "y = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(z)  # 16x16\n",
        "y = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(y)  # 32x32\n",
        "recon = layers.Conv2DTranspose(3, 3, strides=2, padding='same', activation='sigmoid')(y)  # 64x64\n",
        "\n",
        "ae = models.Model(inp, recon, name='autoencoder')\n",
        "ae.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n",
        "ae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX20FpZaX426"
      },
      "outputs": [],
      "source": [
        "history = ae.fit(ae_ds, epochs=EPOCHS_AE, verbose=1)\n",
        "ae.save('ae_keras_savedmodel')\n",
        "print('Saved ae_keras_savedmodel')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6INHHqDX9Y0"
      },
      "source": [
        "# Classifier: reuse encoder + GAP + Dense softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-bjSdglV_ur"
      },
      "outputs": [],
      "source": [
        "encoder = models.Model(inp, z, name='encoder')\n",
        "clf_in = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='input')\n",
        "feat = encoder(clf_in)\n",
        "gap  = tf.keras.layers.GlobalAveragePooling2D()(feat)\n",
        "logits = tf.keras.layers.Dense(N_CLASSES, activation=None, name='logits')(gap)\n",
        "prob   = tf.keras.layers.Softmax(name='softmax')(logits)\n",
        "clf = tf.keras.Model(clf_in, prob, name='classifier')\n",
        "clf.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "clf.summary()\n",
        "hist = clf.fit(train_ds, epochs=EPOCHS_CLF, validation_data=test_ds, verbose=1)\n",
        "y_true, y_pred = [], []\n",
        "for x, y in test_ds:\n",
        "    p = clf.predict(x, verbose=0)\n",
        "    y_true += y.numpy().tolist()\n",
        "    y_pred += p.argmax(axis=1).tolist()\n",
        "macro = f1_score(y_true, y_pred, average='macro')\n",
        "print('Macro-F1:', macro)\n",
        "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "clf.save('clf_keras_savedmodel')\n",
        "print('Saved clf_keras_savedmodel')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
