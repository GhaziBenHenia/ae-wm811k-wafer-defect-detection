# WM811K Wafer Defect Detection

This project explores **image-based defect detection** on semiconductor wafers using the **WM811K Wafer Map** dataset.  
It combines **C++ image preprocessing** and **Python deep learning (Autoencoder + Classifier)** to build a compact, interpretable, and efficient detection pipeline.

The goal is to automatically recognize and categorize wafer failure patterns.


## Overview

In semiconductor manufacturing, every wafer contains thousands of tiny chips (dies).  
After electrical testing, the pass/fail results of each die are visualized as a **wafer map**, an image generated by specialized inspection equipment, where colors or intensities indicate defect locations and patterns such as *Center failures*, *Edge rings*, or *Scratches*.

Instead of relying only on human inspection or rule-based detection, this project applies **machine learning and image processing** to:
- Process and standardize wafer map data (C++)  
- Learn representations of wafer patterns (Autoencoder)  
- Classify wafers into defect types (Neural Network Classifier)  
---

## Pipeline Structure
```
data/LSWMD.pkl ← Original Kaggle dataset
│
├── data_extraction.py → Extract wafer images + metadata
├── preprocessing.cpp → C++ image preprocessing: normalization, masks, distance maps
│
└── WM811K_Autoencoder.ipynb → Autoencoder + Classifier (Keras/TensorFlow)
```

## How to Run

### 1. **Data Extraction (Python)**
After downloading the dataset from Kaggle, place the `LSWMD.pkl` file in the `data/` folder, install any missing python packages.  
Then, run:

```bash
python data_extraction.py
```
This script extracts wafer maps and metadata from the .pkl file and saves them as organized .png images by defect type.

### 2. Preprocessing (C++)
Run the C++ preprocessing step to clean and enhance the extracted wafer maps.
It performs:

  + Intensity normalization
  + Edge and center distance channel computation
  + Bad die mask generation

Use Conan and CMake for easy package installation and building:

### 3. Representation Learning (Autoencoder)
Open and run the notebook:
```
WM811K_Autoencoder.ipynb
```
This trains a convolutional autoencoder to learn low-dimensional representations of wafer maps, capturing the geometric and spatial structure of defect patterns.

### 5. Classification
Within the same notebook, the trained encoder is reused as the backbone for a simple classifier that identifies nine major defect types.
The notebook also handles evaluation, reporting metrics such as accuracy and macro-F1 score.

---
## Data Preparation and Balancing

Because the original dataset is **highly imbalanced**, additional preprocessing was done to create a more representative and balanced subset:

* **Downsampled the "None" class** to 20,000 samples.
* **Upsampled smaller defect classes** (like *Donut*, *Random*, *Scratch*, *Near-Full*) to at least 3,000 samples each.

This adjustment made the classifier learn more evenly across classes and led to a significant performance boost.
However, upsampling introduces some limitations since duplicated samples may slightly reduce data diversity, which can make the model less reliable when generalized to unseen wafers.
Despite that, results showed much better recall for rare classes and an overall more stable classifier.

---

## Model Architectures and Experiments

Three major classifier training setups were explored:

1. **Frozen Encoder + Classifier**

   * The encoder from the trained autoencoder was frozen and only the classifier head was trained.
   * Achieved: **Accuracy = 0.92**, **Macro-F1 = 0.67**.
   * Provided stable performance and good reconstruction consistency.

2. **Unfrozen Encoder + Classifier (joint training)**

   * Both encoder and classifier were trainable from the start.
   * Achieved the **best performance** with **Accuracy = 0.96** and **Macro-F1 = 0.75**.
   * Showed a large improvement over the baseline (0.91 / 0.57) thanks to the data balancing and deeper architecture.

3. **Classifier Without Encoder**

   * Tested as a control experiment using raw images only.
   * Underperformed, confirming that the autoencoder’s learned representation is crucial for defect pattern recognition.

Additionally, the **autoencoder architecture** was expanded (more filters, batch normalization) to improve feature quality.
Reconstruction losses remained low and stable across runs, confirming robust feature extraction.

---

## Results Summary

| Experiment                    | Encoder  | Accuracy | Macro-F1 | Notes                         |
| ----------------------------- | -------- | -------- | -------- | ----------------------------- |
| Baseline (original)           | small AE | 0.91     | 0.57     | simple architecture           |
| Frozen Encoder + Classifier   | big AE   | 0.92     | 0.67     | stable but limited adaptation |
| Unfrozen Encoder + Classifier | big AE   | **0.96** | **0.75** | best performing               |
| Classifier only               | none     | lower    | lower    | confirms encoder importance   |

These results show that both architectural scaling and balanced sampling significantly improved learning.
The larger autoencoder helped capture spatial patterns more effectively, while unfreezing the encoder allowed fine-tuning of features for classification.

---

## Example Wafer Maps

| Example | Description |
|----------|--------------|
| ![Center](samples/example_center.jpg) | Center defect pattern |
| ![Edge-Ring](samples/example_edge_ring.jpg) | Edge-Ring defect pattern |
| ![Scratch](samples/example_scratch.jpg) | Scratch defect pattern |

---

## Next Steps

- Implementing a **C++ rule-based detection module** for fast defect localization and comparison against the ML approach.  
- Exploring **fine-tuning** of the encoder for mixed-pattern wafers.  
- Adding **visual analytics** (heatmaps or reconstruction difference) for better interpretability.

---

## Reference

Dataset: [WM811K Wafer Map (Kaggle)](https://www.kaggle.com/datasets/qingyi/wm811k-wafer-map)

> **Wu, Ming-Ju, Jyh-Shing R. Jang, and Jui-Long Chen.**  
> *“Wafer Map Failure Pattern Recognition and Similarity Ranking for Large-Scale Data Sets.”*  
> *IEEE Transactions on Semiconductor Manufacturing 28(1), 2015.*
